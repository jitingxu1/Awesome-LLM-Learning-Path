## RLHF
- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
- [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267.pdf)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model
](https://arxiv.org/abs/2305.18290)

## Tools
[A modular RL library to fine-tune language models to human preferences](https://github.com/allenai/RL4LMs)
