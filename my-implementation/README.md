# Learn transformer
Transformer 
# Attention is all your need
[Attention explained](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
## Problems Solved
- RNN suffers from long range dependency issue. It cannot handle long sequence well. 
    - Attention mechanism could create weighted connections between any part in the sequence. 
- RNN suffers from gradient vanishing and explosion. 
    - The [residual connections](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55) (a.k.a skip connections)
- Transformer could do parallel computing

## Attention Mechanism
## Self-Attention
## Query, Key and Value 
## Multiheads Attention 

## References
- [Attention explained](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
- [residual connections](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)
- [transformer-walkthrough](https://github.com/markriedl/transformer-walkthrough/tree/main)




