# Why?
Implementing a Transformer model from scratch, despite the availability of excellent implementations, can be a valuable learning experience. Here are a few reasons why I want to implement a Transformer:

- Deepen your understanding: Implementing a complex model like the Transformer from scratch allows you to gain a deeper understanding of its inner workings. It provides an opportunity to grasp the concepts and mechanisms behind the model at a more fundamental level.

- Hands-on learning: "Learning through doing" is an effective way to learn and retain knowledge. By implementing the Transformer by hand, you get hands-on experience with the model's architecture, attention mechanisms, and other components. This practical approach can enhance your understanding and make the concepts more concrete.

- Customization and experimentation: Implementing the Transformer yourself gives you the freedom to customize and experiment with different aspects of the model. You can modify the architecture, introduce new features, or adapt it to suit your specific needs or a particular task. This flexibility allows you to explore variations of the Transformer and gain insights into its behavior.

- Problem-solving skills: Implementing a complex model involves tackling challenges and solving problems along the way. It helps develop your problem-solving skills and the ability to debug and troubleshoot issues. This hands-on experience can be invaluable when working on real-world machine learning projects.

In summary, implementing the Transformer by hand provides a unique opportunity for learning, customization, experimentation, problem-solving, and building a solid foundation in deep learning. It allows you to delve deeper into the model and gain practical experience that goes beyond reading papers, blogs, or watching videos.

# Learn transformer
Transformer 
# Attention is all your need
[Attention explained](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
## Problems Solved
- RNN suffers from long range dependency issue. It cannot handle long sequence well. 
    - Attention mechanism could create weighted connections between any part in the sequence. 
- RNN suffers from gradient vanishing and explosion. 
    - The [residual connections](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55) (a.k.a skip connections)
- Transformer could do parallel computing

## Attention Mechanism
## Self-Attention
## Query, Key and Value 
## Multiheads Attention 

## References
- [Attention explained](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
- [residual connections](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)
- [transformer-walkthrough](https://github.com/markriedl/transformer-walkthrough/tree/main)




