## RLHF
- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
- [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267.pdf)


## Tools
[A modular RL library to fine-tune language models to human preferences](https://github.com/allenai/RL4LMs)